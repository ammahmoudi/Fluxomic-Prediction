{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exper_status.dict', 'rb') as f:\n",
    "    exper_status = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_summary.dict', 'rb') as f:\n",
    "    stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T2F': {'baseline_nn': {'valid_time': (1.3893952369689941, 0.0),\n",
       "   'valid_steps': (10.0, 0.0),\n",
       "   'valid_loss': (15.3416839273927, 0.0),\n",
       "   'valid_eval': (-22.76826207417731, 0.0),\n",
       "   'valid_dist': (9.815769779238001e-06, 0.0),\n",
       "   'valid_ineq_max': (0.032298444922559164, 0.0),\n",
       "   'valid_ineq_mean': (0.0014462382263729936, 0.0),\n",
       "   'valid_ineq_num_viol_0': (4198.741935483871, 0.0),\n",
       "   'valid_ineq_num_viol_1': (3680.8548387096776, 0.0),\n",
       "   'valid_ineq_num_viol_2': (141.88709677419354, 0.0),\n",
       "   'valid_eq_max': (0.05094687977025511, 0.0),\n",
       "   'valid_eq_mean': (0.0044429437392230564, 0.0),\n",
       "   'valid_eq_num_viol_0': (5649.0, 0.0),\n",
       "   'valid_eq_num_viol_1': (4942.0, 0.0),\n",
       "   'valid_eq_num_viol_2': (417.0, 0.0),\n",
       "   'valid_raw_time': (0.0009984970092773438, 0.0),\n",
       "   'valid_raw_eval': (-22.768434169347554, 0.0),\n",
       "   'valid_raw_ineq_max': (0.03229174933803204, 0.0),\n",
       "   'valid_raw_ineq_mean': (0.0014462472076048954, 0.0),\n",
       "   'valid_raw_ineq_num_viol_0': (4197.806451612903, 0.0),\n",
       "   'valid_raw_ineq_num_viol_1': (3680.8548387096776, 0.0),\n",
       "   'valid_raw_ineq_num_viol_2': (141.88709677419354, 0.0),\n",
       "   'valid_raw_eq_max': (0.051306227784726585, 0.0),\n",
       "   'valid_raw_eq_mean': (0.004443335301471124, 0.0),\n",
       "   'valid_raw_eq_num_viol_0': (5649.0, 0.0),\n",
       "   'valid_raw_eq_num_viol_1': (4943.0, 0.0),\n",
       "   'valid_raw_eq_num_viol_2': (417.0, 0.0),\n",
       "   'test_time': (1.3873157501220703, 0.0),\n",
       "   'test_steps': (10.0, 0.0),\n",
       "   'test_loss': (15.23589151460623, 0.0),\n",
       "   'test_eval': (-22.768262079391498, 0.0),\n",
       "   'test_dist': (9.814817963928376e-06, 0.0),\n",
       "   'test_ineq_max': (0.032046456513379874, 0.0),\n",
       "   'test_ineq_mean': (0.0014307617971874129, 0.0),\n",
       "   'test_ineq_num_viol_0': (4150.822580645161, 0.0),\n",
       "   'test_ineq_num_viol_1': (3641.8387096774195, 0.0),\n",
       "   'test_ineq_num_viol_2': (138.83870967741936, 0.0),\n",
       "   'test_eq_max': (0.050946898107745206, 0.0),\n",
       "   'test_eq_mean': (0.0044429437426995125, 0.0),\n",
       "   'test_eq_num_viol_0': (5649.0, 0.0),\n",
       "   'test_eq_num_viol_1': (4942.0, 0.0),\n",
       "   'test_eq_num_viol_2': (417.0, 0.0),\n",
       "   'test_raw_time': (0.0, 0.0),\n",
       "   'test_raw_eval': (-22.768434169347554, 0.0),\n",
       "   'test_raw_ineq_max': (0.03203963653368255, 0.0),\n",
       "   'test_raw_ineq_mean': (0.0014307706521455722, 0.0),\n",
       "   'test_raw_ineq_num_viol_0': (4150.3387096774195, 0.0),\n",
       "   'test_raw_ineq_num_viol_1': (3641.8387096774195, 0.0),\n",
       "   'test_raw_ineq_num_viol_2': (138.83870967741936, 0.0),\n",
       "   'test_raw_eq_max': (0.051306227784726585, 0.0),\n",
       "   'test_raw_eq_mean': (0.004443335301471124, 0.0),\n",
       "   'test_raw_eq_num_viol_0': (5649.0, 0.0),\n",
       "   'test_raw_eq_num_viol_1': (4943.0, 0.0),\n",
       "   'test_raw_eq_num_viol_2': (417.0, 0.0),\n",
       "   'train_loss': (22.914813578426763, 0.0),\n",
       "   'train_time': (5.140577077865601, 0.0)},\n",
       "  'baseline_opt_osqp': {'valid_eval': (-0.006183655878068966, 0.0),\n",
       "   'valid_ineq_max': (0.00031431917475913834, 0.0),\n",
       "   'valid_ineq_mean': (6.546208629452032e-07, 0.0),\n",
       "   'valid_ineq_num_viol_0': (9.258064516129032, 0.0),\n",
       "   'valid_ineq_num_viol_1': (0.0, 0.0),\n",
       "   'valid_ineq_num_viol_2': (0.0, 0.0),\n",
       "   'valid_eq_max': (2.8005323105756025e-05, 0.0),\n",
       "   'valid_eq_mean': (6.763315110870361e-07, 0.0),\n",
       "   'valid_eq_num_viol_0': (0.0, 0.0),\n",
       "   'valid_eq_num_viol_1': (0.0, 0.0),\n",
       "   'valid_eq_num_viol_2': (0.0, 0.0),\n",
       "   'test_eval': (-0.006183655878068966, 0.0),\n",
       "   'test_ineq_max': (0.00031431917475913834, 0.0),\n",
       "   'test_ineq_mean': (6.442036467223166e-07, 0.0),\n",
       "   'test_ineq_num_viol_0': (9.016129032258064, 0.0),\n",
       "   'test_ineq_num_viol_1': (0.0, 0.0),\n",
       "   'test_ineq_num_viol_2': (0.0, 0.0),\n",
       "   'test_eq_max': (2.8005323105756025e-05, 0.0),\n",
       "   'test_eq_mean': (6.763315110870361e-07, 0.0),\n",
       "   'test_eq_num_viol_0': (0.0, 0.0),\n",
       "   'test_eq_num_viol_1': (0.0, 0.0),\n",
       "   'test_eq_num_viol_2': (0.0, 0.0),\n",
       "   'test_time': (0.01578302921787385, 0.0),\n",
       "   'valid_time': (0.015765186279050766, 0.0),\n",
       "   'train_time': (0.0, 0.0),\n",
       "   'test_time_total': (0.9785478115081787, 0.0),\n",
       "   'valid_time_total': (0.9774415493011475, 0.0),\n",
       "   'train_time_total': (0.0, 0.0)}}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex(df):\n",
    "    print(df.to_latex().replace('\\$', '$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_table(results_dict, metrics, keep_methods):\n",
    "    d = {}\n",
    "    missing_methods = []\n",
    "    for method in keep_methods:\n",
    "        if method in results_dict:\n",
    "            d[method] = ['{} ({})'.format(*results_dict[method][metric]) if 'time' in metric else \\\n",
    "                         '{} ({})'.format(*results_dict[method][metric]) for metric in metrics]\n",
    "        else:\n",
    "            missing_methods.append(method)\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    # df.columns = metrics\n",
    "    df.index.names = ['alg']\n",
    "    if len(missing_methods) > 0:\n",
    "        print('missing methods: {}'.format(missing_methods))\n",
    "    return df.loc[[x for x in keep_methods if x not in missing_methods]], missing_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_results_table(results_dict, exper_type, num=5):\n",
    "    metrics = ['test_eval', \n",
    "               'test_eq_max', 'test_eq_mean', \n",
    "               'test_ineq_max', 'test_ineq_mean',\n",
    "               'test_time']\n",
    "    metrics_raw = [x.replace('test_', 'test_raw_') for x in metrics]\n",
    "    metrics_renaming = ['Obj. value', \n",
    "                        'Max eq viol.', 'Mean eq viol.', \n",
    "                        'Max ineq viol.', 'Mean ineq viol.',\n",
    "                        'Time']\n",
    "    \n",
    "    nn_methods = ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft', \n",
    "               'baseline_nn', 'baseline_eq_nn']\n",
    "    opt_methods = dict([\n",
    "         ('T2F',['osqp'])\n",
    "    ])\n",
    "    opt_methods_list = ['baseline_opt_{}'.format(x) for x in opt_methods[exper_type]]\n",
    "    methods_renaming_dict = dict((\n",
    "            ('method', 'DC3'),\n",
    "            ('method_no_compl', 'DC3, $\\neq$'),\n",
    "            ('method_no_corr', 'DC3, $\\not\\leq$ train'),\n",
    "            ('method_no_corr-noTestCorr', 'DC3, $\\not\\leq$ train/test'),\n",
    "            ('method_no_soft', 'DC3, no soft loss'),\n",
    "            ('baseline_nn-noTestCorr', 'NN'),\n",
    "            ('baseline_nn', 'NN, $\\leq$ test'),\n",
    "            ('baseline_eq_nn-noTestCorr', 'Eq.~NN'),\n",
    "            ('baseline_eq_nn', 'Eq.~NN, $\\leq$ test'),\n",
    "            ('baseline_opt_osqp', 'Optimizer (OSQP)'),\n",
    "            ('baseline_opt_qpth', 'Optimizer (qpth)'),\n",
    "            ('baseline_opt_ipopt', 'Optimizer (IPOPT)'),\n",
    "            ('baseline_opt_pypower', 'Optimizer (PYPOWER)')\n",
    "        ))\n",
    "    \n",
    "    df1, missing_methods = get_results_table(results_dict, metrics, nn_methods + opt_methods_list)\n",
    "    \n",
    "    df2, missing_methods_2 = get_results_table(results_dict, metrics_raw, nn_methods)\n",
    "    # df2.columns = metrics\n",
    "    df2.index =  ['{}-noTestCorr'.format(x) for x in df2.index]\n",
    "    \n",
    "    missing_methods = missing_methods + ['{}-noTestCorr'.format(x) for x in missing_methods_2]\n",
    "        \n",
    "    all_results = pd.concat([df1, df2])\n",
    "    results_ordering = opt_methods_list + ['method', 'method_no_compl', 'method_no_corr', 'method_no_corr-noTestCorr',\n",
    "                       'method_no_soft', 'baseline_nn-noTestCorr', 'baseline_nn', 'baseline_eq_nn-noTestCorr', \n",
    "                       'baseline_eq_nn']\n",
    "    all_results = all_results.loc[[x for x in results_ordering if x not in missing_methods]]\n",
    "    all_results.index = [methods_renaming_dict[x] for x in all_results.index]\n",
    "    all_results.columns = metrics_renaming\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing methods: ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft', 'baseline_eq_nn']\n",
      "missing methods: ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft', 'baseline_eq_nn']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Obj. value</th>\n",
       "      <th>Max eq viol.</th>\n",
       "      <th>Mean eq viol.</th>\n",
       "      <th>Max ineq viol.</th>\n",
       "      <th>Mean ineq viol.</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Optimizer (OSQP)</th>\n",
       "      <td>-0.006183655878068966 (0.0)</td>\n",
       "      <td>2.8005323105756025e-05 (0.0)</td>\n",
       "      <td>6.763315110870361e-07 (0.0)</td>\n",
       "      <td>0.00031431917475913834 (0.0)</td>\n",
       "      <td>6.442036467223166e-07 (0.0)</td>\n",
       "      <td>0.01578302921787385 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>-22.768434169347554 (0.0)</td>\n",
       "      <td>0.051306227784726585 (0.0)</td>\n",
       "      <td>0.004443335301471124 (0.0)</td>\n",
       "      <td>0.03203963653368255 (0.0)</td>\n",
       "      <td>0.0014307706521455722 (0.0)</td>\n",
       "      <td>0.0 (0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN, $\\leq$ test</th>\n",
       "      <td>-22.768262079391498 (0.0)</td>\n",
       "      <td>0.050946898107745206 (0.0)</td>\n",
       "      <td>0.0044429437426995125 (0.0)</td>\n",
       "      <td>0.032046456513379874 (0.0)</td>\n",
       "      <td>0.0014307617971874129 (0.0)</td>\n",
       "      <td>1.3873157501220703 (0.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Obj. value                  Max eq viol.  \\\n",
       "Optimizer (OSQP)  -0.006183655878068966 (0.0)  2.8005323105756025e-05 (0.0)   \n",
       "NN                  -22.768434169347554 (0.0)    0.051306227784726585 (0.0)   \n",
       "NN, $\\leq$ test     -22.768262079391498 (0.0)    0.050946898107745206 (0.0)   \n",
       "\n",
       "                                Mean eq viol.                Max ineq viol.  \\\n",
       "Optimizer (OSQP)  6.763315110870361e-07 (0.0)  0.00031431917475913834 (0.0)   \n",
       "NN                 0.004443335301471124 (0.0)     0.03203963653368255 (0.0)   \n",
       "NN, $\\leq$ test   0.0044429437426995125 (0.0)    0.032046456513379874 (0.0)   \n",
       "\n",
       "                              Mean ineq viol.                       Time  \n",
       "Optimizer (OSQP)  6.442036467223166e-07 (0.0)  0.01578302921787385 (0.0)  \n",
       "NN                0.0014307706521455722 (0.0)                  0.0 (0.0)  \n",
       "NN, $\\leq$ test   0.0014307617971874129 (0.0)   1.3873157501220703 (0.0)  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simple_5050 = get_full_results_table(stats['T2F'], exper_type='T2F')\n",
    "df_simple_5050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix tables (simple problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(all_stats, experiments, metrics, keep_methods):\n",
    "    d = {}\n",
    "    for method in keep_methods:\n",
    "        for metric in metrics:\n",
    "            d[(method, metric)] = {}\n",
    "            for experiment in experiments:\n",
    "                d[(method, metric)][experiment] = '{:.2f} ({:.2f})'.format(*all_stats[experiment][method][metric])\n",
    "        df = pd.DataFrame.from_dict(d, orient='index')           \n",
    "    df.index.names = ['Alg', 'Metric']\n",
    "    return df.iloc[df.index.isin(keep_methods, 'Alg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_comparison_table(all_stats, vary_across):\n",
    "    assert vary_across in ['ineq', 'eq'], 'vary_across should be in [ineq, eq]'\n",
    "    \n",
    "    if vary_across == 'ineq':\n",
    "        experiments = ['simple_ineq10_eq50', 'simple_ineq30_eq50', 'simple_ineq50_eq50', 'simple_ineq70_eq50',\n",
    "                       'simple_ineq90_eq50']\n",
    "        experiments_renaming = ['10', '30', '50', '70', '90']\n",
    "    else:\n",
    "        experiments = ['simple_ineq50_eq10', 'simple_ineq50_eq30', 'simple_ineq50_eq50', 'simple_ineq50_eq70',\n",
    "                       'simple_ineq50_eq90']\n",
    "        experiments_renaming = ['10', '30', '50', '70', '90']\n",
    "    \n",
    "    methods = ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft', \n",
    "               'baseline_nn', 'baseline_eq_nn', 'baseline_opt_osqp']\n",
    "    methods_renaming_dict = dict((\n",
    "            ('method', 'DC3'),\n",
    "            ('method_no_compl', 'DC3, $\\neq$'),\n",
    "            ('method_no_corr', 'DC3, $\\not\\leq$ train'),\n",
    "            ('method_no_corr-noTestCorr', 'DC3, $\\not\\leq$ train/test'),\n",
    "            ('method_no_soft', 'DC3, no soft loss'),\n",
    "            ('baseline_nn-noTestCorr', 'NN'),\n",
    "            ('baseline_nn', 'NN, $\\leq$ test'),\n",
    "            ('baseline_eq_nn-noTestCorr', 'Eq.~NN'),\n",
    "            ('baseline_eq_nn', 'Eq.~NN, $\\leq$ test'),\n",
    "            ('baseline_opt_osqp', 'Optimizers (OSQP, qpth)')\n",
    "        ))\n",
    "    \n",
    "    metrics = ['test_eval', 'test_eq_max', 'test_ineq_max']\n",
    "    metrics_renaming_dict = dict((('test_eval', 'Obj. val.'), ('test_eq_max', 'Max eq.'), ('test_ineq_max', 'Max ineq.')  ))\n",
    "    clean_metrics = [metrics_renaming_dict[x] for x in metrics]\n",
    "    metrics_raw = [x.replace('test_', 'test_raw_') if 'time' not in x else x for x in metrics]\n",
    "    \n",
    "    df1 = get_comparison_table(all_stats, experiments, metrics, methods)\n",
    "    df1 = df1.reset_index()\n",
    "    df1['Metric'] = df1['Metric'].str.replace('_obj_val', '_eval') # make naming consistent\n",
    "    df1 = df1.set_index(['Alg', 'Metric'])\n",
    "    \n",
    "    df2 = get_comparison_table(all_stats, experiments, metrics_raw, methods[:-1])\n",
    "    df2.columns = experiments\n",
    "    df2 = df2.reset_index()\n",
    "    df2['Alg'] = ['{}-noTestCorr'.format(x) for x in df2['Alg']]\n",
    "    df2['Metric'] = df2['Metric'].str.replace('raw_', '')  # make naming consistent\n",
    "    df2 = df2.set_index(['Alg', 'Metric'])\n",
    "    \n",
    "    all_results = pd.concat([df1, df2])\n",
    "    results_ordering = ['baseline_opt_osqp', 'method', 'method_no_compl', 'method_no_corr', 'method_no_corr-noTestCorr',\n",
    "                       'method_no_soft', 'baseline_nn-noTestCorr', 'baseline_nn', 'baseline_eq_nn-noTestCorr', \n",
    "                       'baseline_eq_nn']\n",
    "    clean_results_ordering = [methods_renaming_dict[x] for x in results_ordering]\n",
    "    all_results = all_results.iloc[all_results.index.isin(results_ordering, 'Alg')]\n",
    "    all_results = all_results.reindex(pd.MultiIndex.from_product([results_ordering, metrics], names=['Alg', 'Metric']))\n",
    "    all_results = all_results.reset_index()\n",
    "    all_results['Alg'] = [methods_renaming_dict[x] for x in all_results['Alg']]\n",
    "    all_results['Metric'] = [metrics_renaming_dict[x] for x in all_results['Metric']]\n",
    "    all_results = all_results.set_index(['Alg', 'Metric'])\n",
    "    all_results.columns = experiments_renaming\n",
    "    all_results = all_results.reindex(pd.MultiIndex.from_product([clean_results_ordering, clean_metrics])) # remove names\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'simple_ineq50_eq10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\1019552689.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_simple_varyeq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_simple_comparison_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_simple_varyeq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\472449413.py\u001b[0m in \u001b[0;36mget_simple_comparison_table\u001b[1;34m(all_stats, vary_across)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmetrics_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_raw_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'time'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_comparison_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_obj_val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_eval'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make naming consistent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\247213052.py\u001b[0m in \u001b[0;36mget_comparison_table\u001b[1;34m(all_stats, experiments, metrics, keep_methods)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{:.2f} ({:.2f})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mall_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Alg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'simple_ineq50_eq10'"
     ]
    }
   ],
   "source": [
    "df_simple_varyeq = get_simple_comparison_table(stats, 'eq')\n",
    "df_simple_varyeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'simple_ineq10_eq50'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\3285270539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_simple_varyineq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_simple_comparison_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ineq'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_simple_varyineq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\472449413.py\u001b[0m in \u001b[0;36mget_simple_comparison_table\u001b[1;34m(all_stats, vary_across)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmetrics_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test_raw_'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'time'\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_comparison_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_obj_val'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_eval'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make naming consistent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28720\\247213052.py\u001b[0m in \u001b[0;36mget_comparison_table\u001b[1;34m(all_stats, experiments, metrics, keep_methods)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{:.2f} ({:.2f})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mall_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Alg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Metric'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'simple_ineq10_eq50'"
     ]
    }
   ],
   "source": [
    "df_simple_varyineq = get_simple_comparison_table(stats, 'ineq')\n",
    "df_simple_varyineq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "{} &                   Obj. value &                  Max eq viol. &                Mean eq viol. &                Max ineq viol. &              Mean ineq viol. &                       Time \\\\\n",
      "\\midrule\n",
      "Optimizer (OSQP) &  -0.006183655878068966 (0.0) &  2.8005323105756025e-05 (0.0) &  6.763315110870361e-07 (0.0) &  0.00031431917475913834 (0.0) &  6.442036467223166e-07 (0.0) &  0.01578302921787385 (0.0) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP OMEN\\AppData\\Local\\Temp\\ipykernel_28720\\761946459.py:2: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df.to_latex().replace('\\$', '$'))\n"
     ]
    }
   ],
   "source": [
    "get_latex(df_simple_5050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_simple_varyeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_simple_varyineq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
